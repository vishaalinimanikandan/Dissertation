{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526a0223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4V Chart Extraction Relative Accuracy Analysis (FIXED)\n",
      "============================================================\n",
      "Analysis Date: 2025-07-30 13:06:08\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Fixed Relative Accuracy Analysis - Handles Format Mismatch\n",
    "# \n",
    "# This version correctly handles the format differences between extractions and ground truth\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set paths\n",
    "EXTRACTION_PATH = Path(r\"E:\\langchain\\Dissertation\\data\\extractions\")\n",
    "GROUND_TRUTH_PATH = Path(r\"E:\\langchain\\Dissertation\\data\\ground_truth\")\n",
    "RESULTS_PATH = Path(r\"E:\\langchain\\Dissertation\\results\")\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"GPT-4V Chart Extraction Relative Accuracy Analysis (FIXED)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c589e",
   "metadata": {},
   "source": [
    "#### SECTION 1. Load Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55ac3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1399 extraction results\n",
      "Loaded 200 ground truth entries\n"
     ]
    }
   ],
   "source": [
    "def load_extraction_results():\n",
    "    \"\"\"Load all extraction results from JSON files\"\"\"\n",
    "    extractions = {}\n",
    "    json_files = list(EXTRACTION_PATH.glob(\"*.json\"))\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                extractions[json_file.stem] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nLoaded {len(extractions)} extraction results\")\n",
    "    return extractions\n",
    "\n",
    "def load_ground_truth():\n",
    "    \"\"\"Load ground truth data from chart_configurations.json\"\"\"\n",
    "    ground_truth = {}\n",
    "    \n",
    "    config_file = GROUND_TRUTH_PATH / \"chart_configurations.json\"\n",
    "    if config_file.exists():\n",
    "        with open(config_file, 'r') as f:\n",
    "            configs = json.load(f)\n",
    "            for config in configs:\n",
    "                ground_truth[config['id']] = config\n",
    "        print(f\"Loaded {len(ground_truth)} ground truth entries\")\n",
    "    else:\n",
    "        print(\"WARNING: chart_configurations.json not found!\")\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "# Load data\n",
    "extractions = load_extraction_results()\n",
    "ground_truth = load_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773a981",
   "metadata": {},
   "source": [
    "#### 2. Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82466a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_extraction_data(extracted_data):\n",
    "    \"\"\"Normalize extraction data to standard format\"\"\"\n",
    "    normalized = []\n",
    "    \n",
    "    if isinstance(extracted_data, list):\n",
    "        for item in extracted_data:\n",
    "            if isinstance(item, dict) and 'category' in item and 'value' in item:\n",
    "                # Split combined category-series format\n",
    "                category = item['category']\n",
    "                value = float(item['value']) if item['value'] is not None else 0.0\n",
    "                normalized.append({'category': category, 'value': value})\n",
    "    \n",
    "    elif isinstance(extracted_data, dict):\n",
    "        for key, value in extracted_data.items():\n",
    "            normalized.append({'category': key, 'value': float(value) if value is not None else 0.0})\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def normalize_ground_truth_data(gt_config):\n",
    "    \"\"\"Convert ground truth to same format as extraction\"\"\"\n",
    "    normalized = []\n",
    "    \n",
    "    # Handle the categories + series_data format\n",
    "    if 'categories' in gt_config and 'series_data' in gt_config:\n",
    "        categories = gt_config['categories']\n",
    "        series_data = gt_config['series_data']\n",
    "        \n",
    "        # Check if multi-series\n",
    "        if series_data and isinstance(series_data[0], dict) and 'name' in series_data[0]:\n",
    "            # Multi-series format\n",
    "            for series in series_data:\n",
    "                series_name = series['name']\n",
    "                values = series['values']\n",
    "                for i, cat in enumerate(categories):\n",
    "                    if i < len(values):\n",
    "                        # Create combined category name to match extraction format\n",
    "                        combined_cat = f\"{cat} - {series_name}\"\n",
    "                        normalized.append({'category': combined_cat, 'value': float(values[i])})\n",
    "        else:\n",
    "            # Single series - just categories and values\n",
    "            if 'values' in gt_config:\n",
    "                values = gt_config['values']\n",
    "            else:\n",
    "                # Try to extract values from series_data\n",
    "                values = series_data[0]['values'] if series_data else []\n",
    "            \n",
    "            for i, cat in enumerate(categories):\n",
    "                if i < len(values):\n",
    "                    normalized.append({'category': cat, 'value': float(values[i])})\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def calculate_exact_match_fixed(extracted, expected):\n",
    "    \"\"\"Calculate exact match with normalized data\"\"\"\n",
    "    if not extracted or not expected:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize both datasets\n",
    "    extracted_norm = normalize_extraction_data(extracted)\n",
    "    expected_norm = normalize_ground_truth_data(expected) if isinstance(expected, dict) else expected\n",
    "    \n",
    "    if len(extracted_norm) != len(expected_norm):\n",
    "        return 0.0\n",
    "    \n",
    "    # Create value maps for comparison\n",
    "    extracted_map = {item['category'].lower(): item['value'] for item in extracted_norm}\n",
    "    expected_map = {item['category'].lower(): item['value'] for item in expected_norm}\n",
    "    \n",
    "    # Check if all values match (with tolerance)\n",
    "    if set(extracted_map.keys()) != set(expected_map.keys()):\n",
    "        return 0.0\n",
    "    \n",
    "    for key in expected_map:\n",
    "        if abs(extracted_map.get(key, -999) - expected_map[key]) >= 0.1:\n",
    "            return 0.0\n",
    "    \n",
    "    return 1.0\n",
    "\n",
    "def calculate_f1_score_fixed(extracted, expected):\n",
    "    \"\"\"Calculate F1 score with better matching\"\"\"\n",
    "    if not extracted or not expected:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize data\n",
    "    extracted_norm = normalize_extraction_data(extracted)\n",
    "    expected_norm = normalize_ground_truth_data(expected) if isinstance(expected, dict) else expected\n",
    "    \n",
    "    # Extract values for comparison\n",
    "    extracted_values = [(item['category'].lower(), item['value']) for item in extracted_norm]\n",
    "    expected_values = [(item['category'].lower(), item['value']) for item in expected_norm]\n",
    "    \n",
    "    # Calculate matches\n",
    "    true_positives = 0\n",
    "    \n",
    "    for e_cat, e_val in extracted_values:\n",
    "        for g_cat, g_val in expected_values:\n",
    "            # Flexible matching: partial category match OR exact value match\n",
    "            cat_match = any(part in e_cat for part in g_cat.split()) or any(part in g_cat for part in e_cat.split())\n",
    "            val_match = abs(e_val - g_val) < 0.1\n",
    "            \n",
    "            if cat_match and val_match:\n",
    "                true_positives += 1\n",
    "                break\n",
    "    \n",
    "    precision = true_positives / len(extracted_values) if extracted_values else 0\n",
    "    recall = true_positives / len(expected_values) if expected_values else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def calculate_value_accuracy_fixed(extracted, expected):\n",
    "    \"\"\"Calculate value accuracy with 10% tolerance\"\"\"\n",
    "    if not extracted or not expected:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize data\n",
    "    extracted_norm = normalize_extraction_data(extracted)\n",
    "    expected_norm = normalize_ground_truth_data(expected) if isinstance(expected, dict) else expected\n",
    "    \n",
    "    # Get all values\n",
    "    extracted_values = [item['value'] for item in extracted_norm]\n",
    "    expected_values = [item['value'] for item in expected_norm]\n",
    "    \n",
    "    if not expected_values:\n",
    "        return 0.0\n",
    "    \n",
    "    # Match values with tolerance\n",
    "    matched = 0\n",
    "    for e_val in expected_values:\n",
    "        tolerance = abs(e_val * 0.1)  # 10% tolerance\n",
    "        for x_val in extracted_values:\n",
    "            if abs(x_val - e_val) <= tolerance:\n",
    "                matched += 1\n",
    "                break\n",
    "    \n",
    "    return (matched / len(expected_values)) * 100\n",
    "\n",
    "def calculate_chart_type_accuracy_fixed(extracted_type, expected_type):\n",
    "    \"\"\"Fixed chart type matching with common variations\"\"\"\n",
    "    if not extracted_type or not expected_type:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize types\n",
    "    extracted_lower = extracted_type.lower().strip()\n",
    "    expected_lower = expected_type.lower().strip()\n",
    "    \n",
    "    # Direct match\n",
    "    if extracted_lower == expected_lower:\n",
    "        return 1.0\n",
    "    \n",
    "    # Common equivalences\n",
    "    equivalences = {\n",
    "        'bar': ['bar', 'column', 'grouped_bar', 'stacked_bar'],\n",
    "        'line': ['line', 'lines', 'line_chart'],\n",
    "        'scatter': ['scatter', 'scatterplot', 'scatter_plot'],\n",
    "        'pie': ['pie', 'pie_chart'],\n",
    "        'area': ['area', 'area_chart', 'stacked_area']\n",
    "    }\n",
    "    \n",
    "    # Check equivalences\n",
    "    for key, variants in equivalences.items():\n",
    "        if expected_lower in variants and extracted_lower in variants:\n",
    "            return 1.0\n",
    "    \n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b6805",
   "metadata": {},
   "source": [
    "#### 3. Process All Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00a239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing extractions with fixed format handling...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m failed_count = \u001b[32m0\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, extraction \u001b[38;5;129;01min\u001b[39;00m extractions.items():\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     result = \u001b[43mevaluate_extraction_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m     63\u001b[39m         results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mevaluate_extraction_fixed\u001b[39m\u001b[34m(extraction_key, extraction_data, ground_truth_data)\u001b[39m\n\u001b[32m     33\u001b[39m expected_chart_type = gt.get(\u001b[33m'\u001b[39m\u001b[33mchart_type\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Calculate metrics with fixed functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m exact_match = \u001b[43mcalculate_exact_match_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m f1_score = calculate_f1_score_fixed(extracted_data, gt)\n\u001b[32m     38\u001b[39m value_accuracy = calculate_value_accuracy_fixed(extracted_data, gt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mcalculate_exact_match_fixed\u001b[39m\u001b[34m(extracted, expected)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Normalize both datasets\u001b[39;00m\n\u001b[32m     59\u001b[39m extracted_norm = normalize_extraction_data(extracted)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m expected_norm = \u001b[43mnormalize_ground_truth_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expected, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m expected\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(extracted_norm) != \u001b[38;5;28mlen\u001b[39m(expected_norm):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mnormalize_ground_truth_data\u001b[39m\u001b[34m(gt_config)\u001b[39m\n\u001b[32m     26\u001b[39m series_data = gt_config[\u001b[33m'\u001b[39m\u001b[33mseries_data\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Check if multi-series\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m series_data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mseries_data\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m series_data[\u001b[32m0\u001b[39m]:\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Multi-series format\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m series \u001b[38;5;129;01min\u001b[39;00m series_data:\n\u001b[32m     32\u001b[39m         series_name = series[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "def evaluate_extraction_fixed(extraction_key, extraction_data, ground_truth_data):\n",
    "    \"\"\"Evaluate extraction with fixed format handling\"\"\"\n",
    "    \n",
    "    # Parse extraction key\n",
    "    parts = extraction_key.split('_')\n",
    "    chart_id = f\"{parts[0]}_{parts[1]}\"  # e.g., chart_001\n",
    "    \n",
    "    # Determine if original or perturbation\n",
    "    is_original = extraction_key.endswith('_original')\n",
    "    \n",
    "    # Get perturbation type\n",
    "    perturbation_type = 'none'\n",
    "    if not is_original:\n",
    "        # Extract perturbation type from filename\n",
    "        # Format: chart_XXX_[complexity]_[chart_type]_[perturbation]_medium\n",
    "        if len(parts) >= 5:\n",
    "            # Find the perturbation type (skip chart_XXX, complexity, chart_type)\n",
    "            perturbation_candidates = ['gaussian_blur', 'rotation', 'random_blocks', \n",
    "                                     'brightness_shift', 'legend_corruption', 'grayscale_conversion']\n",
    "            for i, part in enumerate(parts[2:], 2):\n",
    "                if part in perturbation_candidates:\n",
    "                    perturbation_type = part\n",
    "                    break\n",
    "    \n",
    "    # Get ground truth\n",
    "    gt = ground_truth_data.get(chart_id, {})\n",
    "    if not gt:\n",
    "        return None\n",
    "    \n",
    "    # Extract data\n",
    "    extracted_data = extraction_data.get('data', [])\n",
    "    extracted_chart_type = extraction_data.get('chart_type', '')\n",
    "    expected_chart_type = gt.get('chart_type', '')\n",
    "    \n",
    "    # Calculate metrics with fixed functions\n",
    "    exact_match = calculate_exact_match_fixed(extracted_data, gt)\n",
    "    f1_score = calculate_f1_score_fixed(extracted_data, gt)\n",
    "    value_accuracy = calculate_value_accuracy_fixed(extracted_data, gt)\n",
    "    chart_type_accuracy = calculate_chart_type_accuracy_fixed(extracted_chart_type, expected_chart_type)\n",
    "    \n",
    "    return {\n",
    "        'extraction_key': extraction_key,\n",
    "        'chart_id': chart_id,\n",
    "        'is_original': is_original,\n",
    "        'perturbation_type': perturbation_type,\n",
    "        'exact_match_accuracy': exact_match,\n",
    "        'f1_score': f1_score,\n",
    "        'value_accuracy': value_accuracy,\n",
    "        'chart_type_accuracy': chart_type_accuracy,\n",
    "        'extraction_confidence': extraction_data.get('extraction_confidence', 'unknown')\n",
    "    }\n",
    "\n",
    "# Process all extractions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing extractions with fixed format handling...\")\n",
    "\n",
    "results = []\n",
    "failed_count = 0\n",
    "\n",
    "for key, extraction in extractions.items():\n",
    "    result = evaluate_extraction_fixed(key, extraction, ground_truth)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "# Add failed extractions\n",
    "expected_total = 1400\n",
    "current_total = len(results)\n",
    "missing_count = expected_total - current_total\n",
    "\n",
    "print(f\"\\nExtraction Summary:\")\n",
    "print(f\"  Expected: {expected_total}\")\n",
    "print(f\"  Processed: {current_total}\")\n",
    "print(f\"  Failed/Missing: {missing_count}\")\n",
    "\n",
    "# Add failed extractions\n",
    "if missing_count > 0:\n",
    "    for i in range(missing_count):\n",
    "        results.append({\n",
    "            'extraction_key': f'failed_extraction_{i}',\n",
    "            'chart_id': 'failed',\n",
    "            'is_original': False,\n",
    "            'perturbation_type': 'failed',\n",
    "            'exact_match_accuracy': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'value_accuracy': 0.0,\n",
    "            'chart_type_accuracy': 0.0,\n",
    "            'extraction_confidence': 'failed'\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5ae56",
   "metadata": {},
   "source": [
    "### 4. Calculate Relative Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fcc452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Calculating Relative Accuracy...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Calculating Relative Accuracy...\")\n",
    "\n",
    "# Calculate composite score for each extraction\n",
    "df_results['composite_score'] = (\n",
    "    df_results['exact_match_accuracy'] + \n",
    "    df_results['f1_score'] + \n",
    "    df_results['value_accuracy'] / 100 +  # Normalize to 0-1 scale\n",
    "    df_results['chart_type_accuracy']\n",
    ") / 4 * 100  # Average of 4 metrics, converted to percentage\n",
    "\n",
    "# Calculate RA for each perturbation\n",
    "ra_results = []\n",
    "\n",
    "for chart_id in df_results['chart_id'].unique():\n",
    "    if chart_id == 'failed':\n",
    "        continue\n",
    "    \n",
    "    # Get original performance\n",
    "    original_data = df_results[(df_results['chart_id'] == chart_id) & \n",
    "                              (df_results['is_original'] == True)]\n",
    "    \n",
    "    if len(original_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    original_score = original_data['composite_score'].iloc[0]\n",
    "    \n",
    "    # Calculate RA for each perturbation of this chart\n",
    "    perturbed_data = df_results[(df_results['chart_id'] == chart_id) & \n",
    "                               (df_results['is_original'] == False)]\n",
    "    \n",
    "    for _, pert_row in perturbed_data.iterrows():\n",
    "        if original_score > 0:\n",
    "            ra = (pert_row['composite_score'] / original_score) * 100\n",
    "        else:\n",
    "            ra = 0.0  # If original failed, RA is 0\n",
    "        \n",
    "        ra_results.append({\n",
    "            'extraction_key': pert_row['extraction_key'],\n",
    "            'relative_accuracy': min(ra, 100.0)  # Cap at 100%\n",
    "        })\n",
    "\n",
    "# Merge RA back to main dataframe\n",
    "ra_df = pd.DataFrame(ra_results)\n",
    "df_results = df_results.merge(ra_df, on='extraction_key', how='left')\n",
    "\n",
    "# Fill RA for originals (always 100%) and failed (always 0%)\n",
    "df_results.loc[df_results['is_original'] == True, 'relative_accuracy'] = 100.0\n",
    "df_results.loc[df_results['perturbation_type'] == 'failed', 'relative_accuracy'] = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d45b0b",
   "metadata": {},
   "source": [
    "### 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef291000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RELATIVE ACCURACY ANALYSIS RESULTS\n",
      "============================================================\n",
      "\n",
      "1. Overall Performance:\n",
      "\n",
      "Original Charts (Clean):\n",
      "  Exact Match: 0.0%\n",
      "  F1 Score: 0.0%\n",
      "  Value Accuracy: 0.0%\n",
      "  Chart Type: 54.0%\n",
      "\n",
      "Perturbed Charts:\n",
      "  Exact Match: 0.0%\n",
      "  F1 Score: 0.0%\n",
      "  Value Accuracy: 0.0%\n",
      "  Chart Type: 54.1%\n",
      "\n",
      "2. Relative Accuracy by Perturbation Type:\n",
      "\n",
      "Perturbation Type    | Mean RA | Std Dev | Count | Interpretation\n",
      "----------------------------------------------------------------------\n",
      "area               |    nan% |    nan% |     0 | Severe Impact\n",
      "bar                |    nan% |    nan% |     0 | Severe Impact\n",
      "line               |    nan% |    nan% |     0 | Severe Impact\n",
      "pie                |    nan% |    nan% |     0 | Severe Impact\n",
      "scatter            |    nan% |    nan% |     0 | Severe Impact\n",
      "\n",
      "Overall Relative Accuracy: nan%\n",
      "Interpretation: GPT-4V retains nan% of its performance on average\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RELATIVE ACCURACY ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n1. Overall Performance:\")\n",
    "originals = df_results[df_results['is_original'] == True]\n",
    "perturbations = df_results[df_results['is_original'] == False]\n",
    "\n",
    "print(f\"\\nOriginal Charts (Clean):\")\n",
    "print(f\"  Exact Match: {originals['exact_match_accuracy'].mean():.1%}\")\n",
    "print(f\"  F1 Score: {originals['f1_score'].mean():.1%}\")\n",
    "print(f\"  Value Accuracy: {originals['value_accuracy'].mean():.1f}%\")\n",
    "print(f\"  Chart Type: {originals['chart_type_accuracy'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nPerturbed Charts:\")\n",
    "print(f\"  Exact Match: {perturbations['exact_match_accuracy'].mean():.1%}\")\n",
    "print(f\"  F1 Score: {perturbations['f1_score'].mean():.1%}\")\n",
    "print(f\"  Value Accuracy: {perturbations['value_accuracy'].mean():.1f}%\")\n",
    "print(f\"  Chart Type: {perturbations['chart_type_accuracy'].mean():.1%}\")\n",
    "\n",
    "# RA by perturbation type\n",
    "print(\"\\n2. Relative Accuracy by Perturbation Type:\")\n",
    "ra_by_type = perturbations.groupby('perturbation_type')['relative_accuracy'].agg(['mean', 'std', 'count'])\n",
    "ra_by_type = ra_by_type.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"\\nPerturbation Type    | Mean RA | Std Dev | Count | Interpretation\")\n",
    "print(\"-\" * 70)\n",
    "for pert_type, row in ra_by_type.iterrows():\n",
    "    if pert_type != 'failed':\n",
    "        interpretation = (\n",
    "            \"Very Robust\" if row['mean'] >= 90 else\n",
    "            \"Robust\" if row['mean'] >= 80 else\n",
    "            \"Moderate\" if row['mean'] >= 70 else\n",
    "            \"Vulnerable\" if row['mean'] >= 60 else\n",
    "            \"Severe Impact\"\n",
    "        )\n",
    "        print(f\"{pert_type:18} | {row['mean']:6.1f}% | {row['std']:6.1f}% | {row['count']:5.0f} | {interpretation}\")\n",
    "\n",
    "# Overall RA\n",
    "overall_ra = perturbations[perturbations['perturbation_type'] != 'failed']['relative_accuracy'].mean()\n",
    "print(f\"\\nOverall Relative Accuracy: {overall_ra:.1f}%\")\n",
    "print(f\"Interpretation: GPT-4V retains {overall_ra:.1f}% of its performance on average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca06f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTION TYPE BREAKDOWN:\n",
      "extraction_type\n",
      "perturbation    698\n",
      "original        200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "PERTURBATION TYPE BREAKDOWN:\n",
      "perturbation_type\n",
      "shift         184\n",
      "blur          180\n",
      "rotation      167\n",
      "blocks         84\n",
      "corruption     83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nEXTRACTION TYPE BREAKDOWN:\")\n",
    "print(df['extraction_type'].value_counts())\n",
    "\n",
    "print(f\"\\nPERTURBATION TYPE BREAKDOWN:\")\n",
    "print(df['perturbation_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fff030",
   "metadata": {},
   "source": [
    "### SECTION 5: ROBUSTNESS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585bbb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 5: ROBUSTNESS ANALYSIS\n",
      " Original evaluations: 200\n",
      " Perturbation evaluations: 698\n",
      "\n",
      "🔍 DETAILED DEBUGGING:\n",
      "ORIGINAL CHART SAMPLE:\n",
      "   Extraction key: chart_179_advanced_bar\n",
      "   Has original_chart_id: True\n",
      "   Original chart id: nan\n",
      "\n",
      "PERTURBATION SAMPLE:\n",
      "   Extraction key: chart_179_advanced_bar_rotation_low\n",
      "   Has original_chart_id: True\n",
      "   Original chart id: chart_179\n",
      "   Original chart id type: <class 'str'>\n",
      "   Is NaN: False\n",
      "\n",
      "AVAILABLE COLUMNS:\n",
      "   Original metrics: ['extraction_key', 'extraction_type', 'exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding', 'original_chart_id', 'perturbation_type', 'intensity']\n",
      "   Perturbation metrics: ['extraction_key', 'extraction_type', 'exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding', 'original_chart_id', 'perturbation_type', 'intensity']\n",
      "\n",
      "NaN CHECK:\n",
      "   Original chart IDs with NaN: 0\n",
      "   Empty original chart IDs: 0\n",
      "\n",
      " DEBUG - Sample original extraction keys:\n",
      "   chart_179_advanced_bar\n",
      "   chart_035_medium_line\n",
      "   chart_058_complex_bar\n",
      "   chart_189_medium_pie\n",
      "   chart_003_medium_bar\n",
      "\n",
      " DEBUG - Sample perturbation original_chart_ids:\n",
      "   chart_179\n",
      "   chart_156\n",
      "   chart_062\n",
      "   chart_123\n",
      "   chart_016\n",
      " Created original lookup with 596 entries\n",
      " Sample lookup keys: ['chart_179', 'chart_179_advanced_bar', 'chart_179_advanced', 'chart_035', 'chart_035_medium_line']\n",
      " Matched 698 perturbations to original charts\n",
      " Created 698 robustness comparisons\n",
      " Robustness analysis saved with 698 records\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 5: ROBUSTNESS ANALYSIS\")\n",
    "\n",
    "def calculate_robustness_metrics_fixed(metrics_df):\n",
    "    \"\"\"Calculate robustness and DRI metrics - FIXED VERSION\"\"\"\n",
    "    \n",
    "    evaluator = StandardMetricsEvaluator()\n",
    "    \n",
    "    # Separate original and perturbation data\n",
    "    original_metrics = metrics_df[metrics_df['extraction_type'] == 'original'].copy()\n",
    "    perturbation_metrics = metrics_df[metrics_df['extraction_type'] == 'perturbation'].copy()\n",
    "    \n",
    "    print(f\" Original evaluations: {len(original_metrics)}\")\n",
    "    print(f\" Perturbation evaluations: {len(perturbation_metrics)}\")\n",
    "    print(\"\\n🔍 DETAILED DEBUGGING:\")\n",
    "\n",
    "    # Check original chart data structure\n",
    "    print(\"ORIGINAL CHART SAMPLE:\")\n",
    "    sample_original = original_metrics.iloc[0]\n",
    "    print(f\"   Extraction key: {sample_original['extraction_key']}\")\n",
    "    print(f\"   Has original_chart_id: {'original_chart_id' in sample_original}\")\n",
    "    if 'original_chart_id' in sample_original:\n",
    "        print(f\"   Original chart id: {sample_original['original_chart_id']}\")\n",
    "\n",
    "    # Check perturbation data structure  \n",
    "    print(\"\\nPERTURBATION SAMPLE:\")\n",
    "    sample_pert = perturbation_metrics.iloc[0]\n",
    "    print(f\"   Extraction key: {sample_pert['extraction_key']}\")\n",
    "    print(f\"   Has original_chart_id: {'original_chart_id' in sample_pert}\")\n",
    "    if 'original_chart_id' in sample_pert:\n",
    "        print(f\"   Original chart id: {sample_pert['original_chart_id']}\")\n",
    "        print(f\"   Original chart id type: {type(sample_pert['original_chart_id'])}\")\n",
    "        print(f\"   Is NaN: {pd.isna(sample_pert['original_chart_id'])}\")\n",
    "\n",
    "    # Check what columns we actually have\n",
    "    print(f\"\\nAVAILABLE COLUMNS:\")\n",
    "    print(f\"   Original metrics: {list(original_metrics.columns)}\")\n",
    "    print(f\"   Perturbation metrics: {list(perturbation_metrics.columns)}\")\n",
    "\n",
    "    # Check for NaN values\n",
    "    print(f\"\\nNaN CHECK:\")\n",
    "    print(f\"   Original chart IDs with NaN: {perturbation_metrics['original_chart_id'].isna().sum()}\")\n",
    "    print(f\"   Empty original chart IDs: {(perturbation_metrics['original_chart_id'] == '').sum()}\")\n",
    "    # DEBUG: Check the actual chart IDs\n",
    "    print(f\"\\n DEBUG - Sample original extraction keys:\")\n",
    "    for key in list(original_metrics['extraction_key'])[:5]:\n",
    "        print(f\"   {key}\")\n",
    "    \n",
    "    print(f\"\\n DEBUG - Sample perturbation original_chart_ids:\")\n",
    "    for oid in list(perturbation_metrics['original_chart_id'])[:5]:\n",
    "        print(f\"   {oid}\")\n",
    "    \n",
    "    # Create original performance lookup with FLEXIBLE matching\n",
    "    original_lookup = {}\n",
    "    \n",
    "    for _, row in original_metrics.iterrows():\n",
    "        extraction_key = row['extraction_key']\n",
    "        \n",
    "        # Extract chart ID from original extraction key\n",
    "        # e.g., \"chart_179_advanced_bar\" -> \"chart_179\"\n",
    "        parts = extraction_key.split('_')\n",
    "        if len(parts) >= 2 and parts[0] == 'chart':\n",
    "            base_chart_id = f\"{parts[0]}_{parts[1]}\"  # chart_179\n",
    "            \n",
    "            # Store with multiple possible keys\n",
    "            possible_keys = [\n",
    "                base_chart_id,                    # chart_179\n",
    "                extraction_key,                   # chart_179_advanced_bar\n",
    "                extraction_key.replace('_original', ''),  # without _original\n",
    "                '_'.join(parts[:3]) if len(parts) >= 3 else base_chart_id  # chart_179_advanced\n",
    "            ]\n",
    "            \n",
    "            for key in possible_keys:\n",
    "                original_lookup[key] = {\n",
    "                    'extraction_key': extraction_key,\n",
    "                    'exact_match_accuracy': row['exact_match_accuracy'],\n",
    "                    'partial_match_f1': row['partial_match_f1'],\n",
    "                    'value_extraction_accuracy': row['value_extraction_accuracy'],\n",
    "                    'structural_understanding': row['structural_understanding']\n",
    "                }\n",
    "    \n",
    "    print(f\" Created original lookup with {len(original_lookup)} entries\")\n",
    "    print(f\" Sample lookup keys: {list(original_lookup.keys())[:5]}\")\n",
    "    \n",
    "    # Calculate robustness metrics for perturbations\n",
    "    robustness_results = []\n",
    "    matched_count = 0\n",
    "    \n",
    "    for _, row in perturbation_metrics.iterrows():\n",
    "        original_chart_id = str(row.get('original_chart_id', ''))\n",
    "        \n",
    "        # Try to find matching original performance\n",
    "        original_perf = None\n",
    "        \n",
    "        # Try exact match first\n",
    "        if original_chart_id in original_lookup:\n",
    "            original_perf = original_lookup[original_chart_id]\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            # Try alternative matching strategies\n",
    "            extraction_key = row['extraction_key']\n",
    "            \n",
    "            # Extract base ID from perturbation extraction key\n",
    "            # e.g., \"chart_179_advanced_bar_gaussian_blur_medium\" -> \"chart_179\"\n",
    "            parts = extraction_key.split('_')\n",
    "            if len(parts) >= 2 and parts[0] == 'chart':\n",
    "                base_id = f\"{parts[0]}_{parts[1]}\"\n",
    "                \n",
    "                # Try different variations\n",
    "                for possible_key in [base_id, original_chart_id, f\"{parts[0]}_{parts[1]}_{parts[2]}\" if len(parts) >= 3 else base_id]:\n",
    "                    if possible_key in original_lookup:\n",
    "                        original_perf = original_lookup[possible_key]\n",
    "                        matched_count += 1\n",
    "                        break\n",
    "        \n",
    "        if original_perf:\n",
    "            # Calculate robustness scores for each metric\n",
    "            robustness_record = {\n",
    "                'extraction_key': row['extraction_key'],\n",
    "                'original_chart_id': original_chart_id,\n",
    "                'matched_original_key': original_perf['extraction_key'],\n",
    "                'perturbation_type': row.get('perturbation_type', ''),\n",
    "                'intensity': row.get('intensity', ''),\n",
    "                \n",
    "                # Current performance\n",
    "                'perturbed_exact_match': row['exact_match_accuracy'],\n",
    "                'perturbed_f1': row['partial_match_f1'],\n",
    "                'perturbed_value_accuracy': row['value_extraction_accuracy'],\n",
    "                'perturbed_structural': row['structural_understanding'],\n",
    "                \n",
    "                # Original performance\n",
    "                'original_exact_match': original_perf['exact_match_accuracy'],\n",
    "                'original_f1': original_perf['partial_match_f1'],\n",
    "                'original_value_accuracy': original_perf['value_extraction_accuracy'],\n",
    "                'original_structural': original_perf['structural_understanding'],\n",
    "            }\n",
    "            \n",
    "            # Calculate DRI scores (handle division by zero)\n",
    "            def safe_dri(original, perturbed):\n",
    "                if original == 0:\n",
    "                    return 1.0 if perturbed == 0 else 0.0\n",
    "                degradation = max(0, original - perturbed)\n",
    "                return max(0.0, min(1.0, 1 - (degradation / original)))\n",
    "            \n",
    "            robustness_record.update({\n",
    "                'dri_exact_match': safe_dri(original_perf['exact_match_accuracy'], row['exact_match_accuracy']),\n",
    "                'dri_f1': safe_dri(original_perf['partial_match_f1'], row['partial_match_f1']),\n",
    "                'dri_value_accuracy': safe_dri(original_perf['value_extraction_accuracy'], row['value_extraction_accuracy']),\n",
    "                'dri_structural': safe_dri(original_perf['structural_understanding'], row['structural_understanding'])\n",
    "            })\n",
    "            \n",
    "            # Calculate composite DRI (PRIMARY METRIC)\n",
    "            dri_scores = [\n",
    "                robustness_record['dri_exact_match'],\n",
    "                robustness_record['dri_f1'],\n",
    "                robustness_record['dri_value_accuracy'],\n",
    "                robustness_record['dri_structural']\n",
    "            ]\n",
    "            robustness_record['composite_dri'] = np.mean(dri_scores)\n",
    "            \n",
    "            robustness_results.append(robustness_record)\n",
    "    \n",
    "    print(f\" Matched {matched_count} perturbations to original charts\")\n",
    "    print(f\" Created {len(robustness_results)} robustness comparisons\")\n",
    "    \n",
    "    if not robustness_results:\n",
    "        print(\" No robustness comparisons created - check ID matching logic\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    robustness_df = pd.DataFrame(robustness_results)\n",
    "    \n",
    "    # Save robustness analysis\n",
    "    robustness_df.to_csv('data/analysis_cache/robustness_analysis.csv', index=False)\n",
    "    print(f\" Robustness analysis saved with {len(robustness_df)} records\")\n",
    "    \n",
    "    return robustness_df\n",
    "# Calculate robustness metrics\n",
    "robustness_df = calculate_robustness_metrics_fixed(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42017677",
   "metadata": {},
   "source": [
    "### SECTION 6: SUMMARY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74547750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 6: SUMMARY STATISTICS\n",
      " PERFORMANCE SUMMARY:\n",
      "------------------------------------------------------------\n",
      " ORIGINAL CHART PERFORMANCE:\n",
      "   exact_match_accuracy: 0.000 ± 0.000\n",
      "   partial_match_f1: 0.114 ± 0.191\n",
      "   value_extraction_accuracy: 8.486 ± 17.430\n",
      "   structural_understanding: 63.001 ± 37.434\n",
      "\n",
      " PERTURBATION PERFORMANCE:\n",
      "   exact_match_accuracy: 0.000 ± 0.000\n",
      "   partial_match_f1: 0.112 ± 0.192\n",
      "   value_extraction_accuracy: 8.249 ± 16.667\n",
      "   structural_understanding: 61.138 ± 36.509\n",
      "\n",
      " ROBUSTNESS ANALYSIS:\n",
      "   Mean Composite DRI: 0.883\n",
      "   DRI Standard Deviation: 0.195\n",
      "   Best DRI Score: 1.000\n",
      "   Worst DRI Score: 0.250\n",
      "\n",
      " PERTURBATION TYPE ANALYSIS:\n",
      "   blocks: DRI = 0.776 ± 0.247 (n=84.0)\n",
      "   blur: DRI = 0.895 ± 0.188 (n=180.0)\n",
      "   corruption: DRI = 0.879 ± 0.188 (n=83.0)\n",
      "   rotation: DRI = 0.896 ± 0.179 (n=167.0)\n",
      "   shift: DRI = 0.910 ± 0.179 (n=184.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 6: SUMMARY STATISTICS\")\n",
    "\n",
    "def generate_summary_statistics(metrics_df, robustness_df):\n",
    "    \"\"\"Generate comprehensive summary statistics\"\"\"\n",
    "    \n",
    "    print(\" PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Original performance\n",
    "    original_data = metrics_df[metrics_df['extraction_type'] == 'original']\n",
    "    \n",
    "    if not original_data.empty:\n",
    "        print(\" ORIGINAL CHART PERFORMANCE:\")\n",
    "        for metric in ['exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding']:\n",
    "            mean_val = original_data[metric].mean()\n",
    "            std_val = original_data[metric].std()\n",
    "            print(f\"   {metric}: {mean_val:.3f} ± {std_val:.3f}\")\n",
    "    \n",
    "    # Perturbation performance\n",
    "    perturbation_data = metrics_df[metrics_df['extraction_type'] == 'perturbation']\n",
    "    \n",
    "    if not perturbation_data.empty:\n",
    "        print(f\"\\n PERTURBATION PERFORMANCE:\")\n",
    "        for metric in ['exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding']:\n",
    "            mean_val = perturbation_data[metric].mean()\n",
    "            std_val = perturbation_data[metric].std()\n",
    "            print(f\"   {metric}: {mean_val:.3f} ± {std_val:.3f}\")\n",
    "    \n",
    "    # Robustness summary\n",
    "    if not robustness_df.empty:\n",
    "        print(f\"\\n ROBUSTNESS ANALYSIS:\")\n",
    "        print(f\"   Mean Composite DRI: {robustness_df['composite_dri'].mean():.3f}\")\n",
    "        print(f\"   DRI Standard Deviation: {robustness_df['composite_dri'].std():.3f}\")\n",
    "        print(f\"   Best DRI Score: {robustness_df['composite_dri'].max():.3f}\")\n",
    "        print(f\"   Worst DRI Score: {robustness_df['composite_dri'].min():.3f}\")\n",
    "        \n",
    "        # Perturbation type analysis\n",
    "        print(f\"\\n PERTURBATION TYPE ANALYSIS:\")\n",
    "        perturbation_summary = robustness_df.groupby('perturbation_type')['composite_dri'].agg(['mean', 'std', 'count'])\n",
    "        for pert_type, stats in perturbation_summary.iterrows():\n",
    "            print(f\"   {pert_type}: DRI = {stats['mean']:.3f} ± {stats['std']:.3f} (n={stats['count']})\")\n",
    "\n",
    "generate_summary_statistics(metrics_df, robustness_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c93c8b",
   "metadata": {},
   "source": [
    "#### SECTION 7: ANALYSIS COMPLETION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e029a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 7: ANALYSIS COMPLETION\n",
      " Analysis summary saved\n",
      " Multi-metric analysis complete!\n",
      " 898 evaluations processed\n",
      " Primary metric: composite_dri (Composite DRI)\n",
      " Academic metrics: 6\n",
      "\n",
      "================================================================================\n",
      " MULTI-METRIC ANALYSIS COMPLETE!\n",
      " Standard Academic Metrics Successfully Calculated\n",
      " Ready for Statistical Analysis Phase\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 7: ANALYSIS COMPLETION\")\n",
    "\n",
    "# Create final analysis summary\n",
    "analysis_summary = {\n",
    "    'analysis_timestamp': datetime.now().isoformat(),\n",
    "    'total_evaluations': len(metrics_df),\n",
    "    'original_evaluations': len(metrics_df[metrics_df['extraction_type'] == 'original']),\n",
    "    'perturbation_evaluations': len(metrics_df[metrics_df['extraction_type'] == 'perturbation']),\n",
    "    'robustness_comparisons': len(robustness_df),\n",
    "    'primary_metric': 'composite_dri',\n",
    "    'metrics_calculated': list(STANDARD_METRICS.keys()),\n",
    "    'academic_references': [info['reference'] for info in STANDARD_METRICS.values()],\n",
    "    'data_files_created': [\n",
    "        'data/analysis_cache/comprehensive_metrics.csv',\n",
    "        'data/analysis_cache/robustness_analysis.csv'\n",
    "    ],\n",
    "    'ready_for_statistical_analysis': True,\n",
    "    'next_notebook': '06_Statistical_Analysis.ipynb'\n",
    "}\n",
    "\n",
    "# Save analysis summary\n",
    "with open('data/analysis_cache/analysis_summary.json', 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(\" Analysis summary saved\")\n",
    "print(f\" Multi-metric analysis complete!\")\n",
    "print(f\" {analysis_summary['total_evaluations']} evaluations processed\")\n",
    "print(f\" Primary metric: {analysis_summary['primary_metric']} (Composite DRI)\")\n",
    "print(f\" Academic metrics: {len(analysis_summary['metrics_calculated'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" MULTI-METRIC ANALYSIS COMPLETE!\")\n",
    "print(\" Standard Academic Metrics Successfully Calculated\")\n",
    "print(\" Ready for Statistical Analysis Phase\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Log completion\n",
    "logger.info(\"Multi-metric analysis completed successfully\")\n",
    "logger.info(f\"Total evaluations: {analysis_summary['total_evaluations']}\")\n",
    "logger.info(f\"Primary metric: {analysis_summary['primary_metric']}\")\n",
    "logger.info(\"Ready for statistical analysis phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ad8339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Robustness analysis file found!\n",
      " Robustness comparisons: 698\n",
      " Mean Composite DRI: 0.883\n",
      "\n",
      " DRI by perturbation type:\n",
      "perturbation_type\n",
      "blocks        0.776\n",
      "corruption    0.879\n",
      "blur          0.895\n",
      "rotation      0.896\n",
      "shift         0.910\n",
      "Name: composite_dri, dtype: float64\n",
      "\n",
      " DRI Distribution:\n",
      "   Best DRI: 1.000\n",
      "   Worst DRI: 0.250\n",
      "   Std Dev: 0.195\n"
     ]
    }
   ],
   "source": [
    "# Run this AFTER Notebook 5 is completely done\n",
    "import pandas as pd\n",
    "\n",
    "# Check if robustness analysis worked\n",
    "try:\n",
    "    df = pd.read_csv('data/analysis_cache/robustness_analysis.csv')\n",
    "    print(f\" Robustness analysis file found!\")\n",
    "    print(f\" Robustness comparisons: {len(df)}\")\n",
    "    print(f\" Mean Composite DRI: {df['composite_dri'].mean():.3f}\")\n",
    "    print(f\"\\n DRI by perturbation type:\")\n",
    "    print(df.groupby('perturbation_type')['composite_dri'].mean().sort_values().round(3))\n",
    "    \n",
    "    print(f\"\\n DRI Distribution:\")\n",
    "    print(f\"   Best DRI: {df['composite_dri'].max():.3f}\")\n",
    "    print(f\"   Worst DRI: {df['composite_dri'].min():.3f}\")\n",
    "    print(f\"   Std Dev: {df['composite_dri'].std():.3f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\" Robustness analysis file not found - Notebook 5 may not have completed Section 5\")\n",
    "except Exception as e:\n",
    "    print(f\" Error reading robustness analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d45067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
